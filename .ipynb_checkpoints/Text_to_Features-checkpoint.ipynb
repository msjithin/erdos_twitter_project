{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Features\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, we load the dataframe `df_tsla OR aapl.csv` of Tweets containing the index names `AAPL` and `TSLA`. We compute the (normalized) number of words from various word libraries for each Tweet in the string format. The word libraries include:\n",
    "- `Henry08_poswords.txt` and `Henry08_negwords.txt`, containing positive and negative words, respectively, from Henry (2008).\n",
    "- `LM11_pos_words.txt` and `LM11_neg_words.txt`, containing words related to positive and negative sentiments, respectively, from Loughran and McDonald (2011).\n",
    "- `ML_positive_bigram.csv` and `ML_negative_bigram.csv`, containing positive and negative bigrams (no trigrams??????), respectively, from Hagenau et al. (2013). \n",
    "- `news_library.txt`, containing names of mainstream business news agencies.\n",
    "\n",
    "The word counts will be added as new columns to the dataframe and will be saved on a new file: `df_tsla_aapl_features_added.csv`, which will be used for our model fit.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "We use `pandas` to for dataframe and `spaCy` for linguistic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `spaCy`'s `en_core_web_sm` model as the underlying English language processing model. Throughout this notebook, denote the model by `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we use `PhraseMatcher` (https://spacy.io/api/phrasematcher) to find word counts in order to be able to work with bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "In this section, we write all the functions necessary to compute the word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_tokens(filename: str):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_tokens(filename: str):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_wordlocs(tweet, keys, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The tweet text in string\n",
    "    keys -> The list of lists of key words/bigrams. For example, keys = [henry08_pos, henry08_neg, ..., newslib]\n",
    "            Each key word/bigram is assumed to contain only English letter and space.\n",
    "    case_sensitive -> If True, match the terms in the case-sensitive manner. \n",
    "    \n",
    "    Output:\n",
    "    wordlocs -> A list whose elements are of the form [keyloc, start, end], such that for the key word/bigram \n",
    "    from the i-th library, keys[i], located in tweet from tweet[j] to tweet[j+k-1] inclusive, we have \n",
    "    keyloc = i, start = j and end = j+k. The elements of wordlocs are sorted based on the value of start.\n",
    "    \"\"\"\n",
    "    # Define the phrase matcher according to input case sensitivity\n",
    "    if case_sensitive:\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "    else:\n",
    "        matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    tweet_text = nlp(tweet)\n",
    "    \n",
    "    # Tokenize the key phrases and introduce them to the phrase matcher model.\n",
    "    for i in range(len(keys)):\n",
    "        phrases = [nlp(phrase) for phrase in keys[i]]\n",
    "        matcher.add(str(i), phrases)\n",
    "    \n",
    "    # Find the matches\n",
    "    matches = matcher(tweet_text)\n",
    "    \n",
    "    # Define and populate wordlocs\n",
    "    wordlocs = []\n",
    "    for i in range(len(matches)):\n",
    "        keyloc_shifted, start, end = matches[i]\n",
    "        keyloc = int(nlp.vocab.strings[keyloc_shifted])\n",
    "        wordlocs.append([keyloc, start, end])\n",
    "    \n",
    "    return wordlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlocs_to_wordcounts(wordlocs, tweet_length, num_keys, normalize=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    wordlocs -> The list of keys and locations found in the tweet, c.f. tweet_to_wordlocs function.\n",
    "    tweet_length -> The length of tweet in words.\n",
    "    num_keys -> The number of phrase lists, i.e. len(keys) from tweet_to_wordlocs function.\n",
    "    normalize -> If True, the word count for each keyword list is normalized by tweet_length.\n",
    "                 If False, the raw word count will be returned.\n",
    "    \n",
    "    Output:\n",
    "    wordcounts -> A list of length num_keys. Each element is the (normalized) word count corresponding\n",
    "                  to the number of phrases from one of the phrase lists that appear in the tweet, \n",
    "                  as reported in wordlocs.\n",
    "    \"\"\"\n",
    "    # Define and populate wordcounts\n",
    "    wordcounts = [0 for i in range(num_keys)]\n",
    "    for j in range(len(wordlocs)):\n",
    "        wordcounts[wordlocs[j][0]] += 1\n",
    "    \n",
    "    # Perform normalization as needed\n",
    "    if normalize:\n",
    "        return [wordcounts[i]/tweet_length for i in range(num_keys)]\n",
    "    \n",
    "    # In the case where normalization is not called for\n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('Twitter_Sentiment_Analysis/Directional_Feature_Libraries/Henry08_negwords.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative negatives fail fails failing failure weak weakness weaknesses difficult difficulty hurdle hurdles obstacle obstacles slump slumps slumping slumped uncertain uncertainty unsettled unfavorable downturn depressed disappoint disappoints disappointing disappointed disappointment risk risks risky threat threats penalty penalties down decrease decreases decreasing decreased decline declines declining declined fall falls falling fell fallen drop drops dropping dropped deteriorate deteriorates deteriorating deteriorated worsen worsens worsening weaken weakens weakening weakened worse worst low lower lowest less least smaller smallest shrink shrinks shrinking shrunk below under challenge challenges challenging challenged\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
