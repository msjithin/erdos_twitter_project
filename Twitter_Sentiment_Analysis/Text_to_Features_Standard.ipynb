{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Features\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, we load the table `tweets_data.csv` of Tweets. We compute the (normalized) number of words from various word libraries for each Tweet in the string format. The word libraries include:\n",
    "- `Henry08_poswords.txt` and `Henry08_negwords.txt`, containing positive and negative words, respectively, from Henry (2008).\n",
    "- `LM11_pos_words.txt` and `LM11_neg_words.txt`, containing words related to positive and negative sentiments, respectively, from Loughran and McDonald (2011).\n",
    "- `ML_positive_bigram.csv` and `ML_negative_bigram.csv`, containing positive and negative bigrams (no trigrams??????), respectively, from Hagenau et al. (2013). \n",
    "- `news_library.txt`, containing names of mainstream business news agencies.\n",
    "Furthermore, we identify the stock indices listed in `snp500_list.csv` and `nyse_list.csv` for each row in the table.\n",
    "\n",
    "The word counts and the list of mentioned stock indices will be added as new columns to the dataframe and will be saved on a new file: `tweets_data_features_added.csv`. The file will be read into the notebook that performs our model fit.\n",
    "\n",
    "This notebook can easily be modified to treat a different tweet table file.\n",
    "\n",
    "\n",
    "### Libraries\n",
    "\n",
    "We use `pandas` for dataframe and `spaCy` for linguistic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `spaCy`'s `en_core_web_sm` model as the underlying English language processing model. Throughout this notebook, denote the model by `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we use `PhraseMatcher` (https://spacy.io/api/phrasematcher) to find word counts in order to conveniently work with bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "For the code in this section to work, one must render the functions in [Helper Functions](#the_destination) section first. The docstrings for helper functions also provide additional details about the method employed.\n",
    "<br><br>\n",
    "Define the local path of your repository folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "localpath = \"/Users/josht/Documents/GitHub/erdos_twitter_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file `tweets_data.csv` into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "#df_tweets = pd.read_csv(localpath + \"/data/tweets_data.csv\")\n",
    "\n",
    "# Run this during the trial run\n",
    "#df_tweets = pd.read_csv(localpath + \"/data/Tweets_Raw/df_tsla OR aapl.csv\")\n",
    "\n",
    "# Starbucks Example\n",
    "df_tweets = pd.read_csv(localpath + \"/data/df_Starbucks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet contents appear as strings in the `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities_cashtags</th>\n",
       "      <th>entities_hashtags</th>\n",
       "      <th>entities_urls</th>\n",
       "      <th>public_metrics_like_count</th>\n",
       "      <th>public_metrics_quote_count</th>\n",
       "      <th>public_metrics_reply_count</th>\n",
       "      <th>public_metrics_retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>entities_mentions</th>\n",
       "      <th>created_at_user</th>\n",
       "      <th>public_metrics_followers_count</th>\n",
       "      <th>public_metrics_following_count</th>\n",
       "      <th>public_metrics_listed_count</th>\n",
       "      <th>public_metrics_tweet_count</th>\n",
       "      <th>media_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-30 19:59:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Campus labor shortage delays opening of the St...</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-26 18:14:59</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-30 19:59:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Yo what are fire Starbucks drinks</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-05-23 16:28:47</td>\n",
       "      <td>96</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-30 19:59:07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-09-16 05:43:14</td>\n",
       "      <td>4151</td>\n",
       "      <td>4962</td>\n",
       "      <td>56</td>\n",
       "      <td>391400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at  entities_cashtags  entities_hashtags  entities_urls  \\\n",
       "0  2021-09-30 19:59:36                  0                  0              2   \n",
       "1  2021-09-30 19:59:12                  0                  0              0   \n",
       "2  2021-09-30 19:59:07                  0                  0              2   \n",
       "\n",
       "   public_metrics_like_count  public_metrics_quote_count  \\\n",
       "0                          4                           0   \n",
       "1                          4                           0   \n",
       "2                          0                           0   \n",
       "\n",
       "   public_metrics_reply_count  public_metrics_retweet_count  \\\n",
       "0                           0                             0   \n",
       "1                           3                             0   \n",
       "2                           0                             0   \n",
       "\n",
       "                                                text  entities_mentions  \\\n",
       "0  Campus labor shortage delays opening of the St...                  1   \n",
       "1                  Yo what are fire Starbucks drinks                  0   \n",
       "2  https://t.co/dTZGW5bmsO\\n\\nhttps://t.co/APZH7I...                  2   \n",
       "\n",
       "       created_at_user  public_metrics_followers_count  \\\n",
       "0  2021-07-26 18:14:59                              21   \n",
       "1  2020-05-23 16:28:47                              96   \n",
       "2  2010-09-16 05:43:14                            4151   \n",
       "\n",
       "   public_metrics_following_count  public_metrics_listed_count  \\\n",
       "0                              28                            0   \n",
       "1                             116                            0   \n",
       "2                            4962                           56   \n",
       "\n",
       "   public_metrics_tweet_count  media_type  \n",
       "0                          35           0  \n",
       "1                         919           0  \n",
       "2                      391400           0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the libraries of key phrases and news agencies names, then put them in a list called `keys`. Each element of `keys` is a set of words from the corresponding library file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_words = [localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/Henry08_poswords.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/Henry08_negwords.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/LM11_pos_words.txt\",\n",
    "                  localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/LM11_neg_words.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_bigrams = [localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/ML_positive_bigram.csv\",\n",
    "                   localpath + \"/Twitter_Sentiment_Analysis/Directional_Feature_Libraries/ML_negative_bigram.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_news = [localpath + \"/Twitter_Sentiment_Analysis/Relevance_Feature_Libraries/news_library.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All libraries\n",
    "keys = [get_keywords(keyfile) for keyfile in keyfiles_words] + [get_keybigrams(keyfile) for keyfile in keyfiles_bigrams] + [get_news_agencies(keyfile) for keyfile in keyfiles_news]\n",
    "\n",
    "# Ignoring bigrams (significantly faster)\n",
    "#keys = [get_keywords(keyfile) for keyfile in keyfiles_words] + [get_news_agencies(keyfile) for keyfile in keyfiles_news]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how many words are there in each library, we compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 85, 354, 2355, 12130, 13330, 23]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(keys[i]) for i in range(len(keys))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the library names in a legible manner for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "key_library = [\"Henry08_pos\", \"Henry08_neg\", \"LM11_pos\", \"LM11_neg\", \"Hagenau13_pos\", \"Hagenau13_neg\", \"News_agencies\"]\n",
    "\n",
    "# Run this during the trial run\n",
    "#key_library = [\"Henry08_pos\", \"Henry08_neg\", \"LM11_pos\", \"LM11_neg\", \"News_agencies\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare the list of stock indices, starting once again from the file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfiles_stocks = [localpath + \"/data/Stock_indices/snp500_list.csv\",\n",
    "                  localpath + \"/data/Stock_indices/nyse_list.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataframe from each csv file, then extract the list of stock indices acronyms from the dataframe. Finally, we store all lists of indices into `stocks`, in a similar manner to `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = []\n",
    "\n",
    "for file in keyfiles_stocks:\n",
    "    df_stocks = pd.read_csv(file)\n",
    "    stocks.append(list(df_stocks[\"Symbol\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `snp500_list.csv`, the company names are available under column `Security`. However, for `nyse_list.csv`, only index names are available under column `Name`. Unfortunately, there is no regular pattern to go from index names to company names. Also, index names are so long and specific that it is probably very rarely mentioned in full on Twitter. However, I am open to an alternative solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = []\n",
    "\n",
    "# Add company names from snp500_list.csv\n",
    "df_stocks = pd.read_csv(keyfiles_stocks[0])\n",
    "company_names.append(list(df_stocks[\"Security\"]))\n",
    "\n",
    "# As a place holder for nyse_list.csv, make it an empty list\n",
    "company_names.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the stock index library names for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_library = [\"S&P500\", \"NYSE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mentioned Stock Indices\n",
    "\n",
    "We apply `get_stock_list` to each tweet in `df_tweets[\"text\"]` and each stock library in `stocks`. We store the stock lists in the list called `mentioned_stocks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_stocks = [[[] for i in range(df_tweets.shape[0])] for j in range(len(stocks))]\n",
    "\n",
    "for j in range(len(stocks)):\n",
    "    for i in range(df_tweets.shape[0]):\n",
    "        mentioned_stocks[j][i] = get_stock_list(df_tweets[\"text\"].iloc[i], stocks[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we put `mentioned_stocks` into corresponding new columns, e.g. `Mentioned_stocks_S&P500`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(stocks)):\n",
    "    df_tweets[\"Mentioned_stocks_\" + stock_library[j]] = mentioned_stocks[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see that new columns are added listing the mentioned stocks. Some rows have empty lists for both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_at_user</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>public_metrics_followers_count</th>\n",
       "      <th>public_metrics_following_count</th>\n",
       "      <th>public_metrics_like_count</th>\n",
       "      <th>public_metrics_listed_count</th>\n",
       "      <th>public_metrics_quote_count</th>\n",
       "      <th>public_metrics_reply_count</th>\n",
       "      <th>public_metrics_retweet_count</th>\n",
       "      <th>public_metrics_tweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "      <th>Mentioned_stocks_S&amp;P500</th>\n",
       "      <th>Mentioned_stocks_NYSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1023266600223481856</td>\n",
       "      <td>2018-12-31T23:41:21.000Z</td>\n",
       "      <td>2018-07-28T17:59:03.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Polixenes</td>\n",
       "      <td>4163</td>\n",
       "      <td>1023</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13389</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>@DeanSheikh1 @Jekajojo @schlosta2 @elonmusk @T...</td>\n",
       "      <td>1079885248568135680</td>\n",
       "      <td>Polixenes13</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24843079</td>\n",
       "      <td>2018-12-31T23:50:05.000Z</td>\n",
       "      <td>2009-03-17T05:04:24.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dan Stringer, SEC Pimp</td>\n",
       "      <td>7509</td>\n",
       "      <td>2107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>117795</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @4Awesometweet: 79% of Electric Vehicle  ta...</td>\n",
       "      <td>1079887447155040256</td>\n",
       "      <td>Danstringer74</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1042888795501289472</td>\n",
       "      <td>2018-12-31T23:43:19.000Z</td>\n",
       "      <td>2018-09-20T21:30:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Capvalue89</td>\n",
       "      <td>14</td>\n",
       "      <td>923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1192</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @ElonBachman: Am I hallucinating? $TSLA jus...</td>\n",
       "      <td>1079885743437295617</td>\n",
       "      <td>CapitalOnValue</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author_id                created_at           created_at_user  \\\n",
       "28  1023266600223481856  2018-12-31T23:41:21.000Z  2018-07-28T17:59:03.000Z   \n",
       "10             24843079  2018-12-31T23:50:05.000Z  2009-03-17T05:04:24.000Z   \n",
       "16  1042888795501289472  2018-12-31T23:43:19.000Z  2018-09-20T21:30:39.000Z   \n",
       "\n",
       "   location                    name  public_metrics_followers_count  \\\n",
       "28      NaN               Polixenes                            4163   \n",
       "10      NaN  Dan Stringer, SEC Pimp                            7509   \n",
       "16      NaN              Capvalue89                              14   \n",
       "\n",
       "    public_metrics_following_count  public_metrics_like_count  \\\n",
       "28                            1023                          2   \n",
       "10                            2107                          0   \n",
       "16                             923                          0   \n",
       "\n",
       "    public_metrics_listed_count  public_metrics_quote_count  \\\n",
       "28                          102                           0   \n",
       "10                            0                           0   \n",
       "16                            0                           0   \n",
       "\n",
       "    public_metrics_reply_count  public_metrics_retweet_count  \\\n",
       "28                           1                             0   \n",
       "10                           0                            22   \n",
       "16                           0                            60   \n",
       "\n",
       "    public_metrics_tweet_count              source  \\\n",
       "28                       13389  Twitter Web Client   \n",
       "10                      117795  Twitter for iPhone   \n",
       "16                        1192  Twitter for iPhone   \n",
       "\n",
       "                                                 text             tweet_id  \\\n",
       "28  @DeanSheikh1 @Jekajojo @schlosta2 @elonmusk @T...  1079885248568135680   \n",
       "10  RT @4Awesometweet: 79% of Electric Vehicle  ta...  1079887447155040256   \n",
       "16  RT @ElonBachman: Am I hallucinating? $TSLA jus...  1079885743437295617   \n",
       "\n",
       "          username Mentioned_stocks_S&P500 Mentioned_stocks_NYSE  \n",
       "28     Polixenes13                      []                    []  \n",
       "10   Danstringer74                      []                    []  \n",
       "16  CapitalOnValue                  [TSLA]                    []  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to save time for future steps, we drop from `df_tweets` the rows whose tweet mentions no stock index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the indices for the rows in which no stock from each stock library is mentioned.\n",
    "emp_ind = []\n",
    "for i in range(df_tweets.shape[0]):\n",
    "    if len(df_tweets[\"Mentioned_stocks_S&P500\"].iloc[i]) == 0 and len(df_tweets[\"Mentioned_stocks_NYSE\"].iloc[i]) == 0:\n",
    "        emp_ind.append(i)\n",
    "\n",
    "df_tweets_shorten = df_tweets.drop(emp_ind).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some rows have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of rows:\", len(df_tweets.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows that mention a stock: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of rows that mention a stock:\", len(df_tweets_shorten.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts\n",
    "\n",
    "We apply `tweet_to_wordcounts` to each tweet in `df_tweets_shorten[\"text\"]` that mentions at least one stock, either through indices or company names. Then, we store the results in `wordcounts_all`. \n",
    "<br><br>\n",
    "Warning: this step may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_all = [[-1 for i in range(df_tweets_shorten.shape[0])] for j in range(len(keys))]\n",
    "\n",
    "for i in range(df_tweets_shorten.shape[0]):\n",
    "    \n",
    "    # Run this if ignoring bigrams\n",
    "    # wordcounts = tweet_to_wordcounts(df_tweets_shorten[\"text\"].iloc[i], keys)\n",
    "    \n",
    "    # Run this if including bigrams\n",
    "    wordcounts = tweet_to_wordcounts(df_tweets_shorten[\"text\"].iloc[i], keys[:4] + [keys[-1]])\n",
    "    bigramcounts = tweet_to_bigramcounts(df_tweets_shorten[\"text\"].iloc[i], keys[4:6])\n",
    "    \n",
    "    for j in range(len(keys)):\n",
    "        if j <= 3:\n",
    "            wordcounts_all[j][i] = wordcounts[j]\n",
    "        elif j == len(keys) - 1:\n",
    "            wordcounts_all[-1][i] = wordcounts[-1]\n",
    "        else:\n",
    "            wordcounts_all[j][i] = bigramcounts[j - 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we put the resulting word counts for each phrase library into the corresponding new column, e.g. `Word_count_Henry08_pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(keys[:-1])):\n",
    "    df_tweets_shorten[\"Word_count_\" + key_library[j]] = wordcounts_all[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the column for the number of news agency names that appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_shorten[\"News_agencies_names_count\"] = wordcounts_all[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Applying all the above operations related to word counts and mentioned stock indices, we modify `df_tweets` to the following form. Note that word counts are normalized, i.e. divided, by the total word count for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Mentioned_stocks_S&amp;P500</th>\n",
       "      <th>Mentioned_stocks_NYSE</th>\n",
       "      <th>Word_count_Henry08_pos</th>\n",
       "      <th>Word_count_Henry08_neg</th>\n",
       "      <th>Word_count_LM11_pos</th>\n",
       "      <th>Word_count_LM11_neg</th>\n",
       "      <th>Word_count_Hagenau13_pos</th>\n",
       "      <th>Word_count_Hagenau13_neg</th>\n",
       "      <th>News_agencies_names_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BLSP huge volume, closes up 12.5%.  Shares st...</td>\n",
       "      <td>[MSFT, AAPL, AMZN, FB, TSLA, BRK.B]</td>\n",
       "      <td>[HRI]</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Polixenes13: Ross, please just never stop ...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kimpaquette I pity the fool that is shorting ...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$TSLA passed 190K model 3 VIN registered. Yeah...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @TeslaCharts: Fraud. Fraud. Fraud. Fraud. \\...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FCC to suspend most operations this week due t...</td>\n",
       "      <td>[AAPL]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://t.co/0HOIOe45er\\n\\n$TSLA news</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Props to @GerberKawasaki for at least spelling...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @stockmarkettv: Tesla Production Numbers So...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bottom Filled on $WDBG Closed up 23% , Ready t...</td>\n",
       "      <td>[MSFT, AAPL, AMZN, FB, TSLA, BRK.B]</td>\n",
       "      <td>[HRI]</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RT @ElonBachman: Am I hallucinating? $TSLA jus...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Amusingly High quality clip not sure about the...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@GerberKawasaki Congrats on a rockin 3.8% gain...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@Gordon_GekkoZ COUGH $TSLA COUGH\\n\\nWhy couldn...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @evannex_com: Does it make sense to pull th...</td>\n",
       "      <td>[TSLA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   $BLSP huge volume, closes up 12.5%.  Shares st...   \n",
       "1   RT @Polixenes13: Ross, please just never stop ...   \n",
       "2   @kimpaquette I pity the fool that is shorting ...   \n",
       "5   $TSLA passed 190K model 3 VIN registered. Yeah...   \n",
       "7   RT @TeslaCharts: Fraud. Fraud. Fraud. Fraud. \\...   \n",
       "8   FCC to suspend most operations this week due t...   \n",
       "9               https://t.co/0HOIOe45er\\n\\n$TSLA news   \n",
       "11  Props to @GerberKawasaki for at least spelling...   \n",
       "12  RT @stockmarkettv: Tesla Production Numbers So...   \n",
       "15  Bottom Filled on $WDBG Closed up 23% , Ready t...   \n",
       "16  RT @ElonBachman: Am I hallucinating? $TSLA jus...   \n",
       "25  Amusingly High quality clip not sure about the...   \n",
       "26  @GerberKawasaki Congrats on a rockin 3.8% gain...   \n",
       "27  @Gordon_GekkoZ COUGH $TSLA COUGH\\n\\nWhy couldn...   \n",
       "29  RT @evannex_com: Does it make sense to pull th...   \n",
       "\n",
       "                Mentioned_stocks_S&P500 Mentioned_stocks_NYSE  \\\n",
       "0   [MSFT, AAPL, AMZN, FB, TSLA, BRK.B]                 [HRI]   \n",
       "1                                [TSLA]                    []   \n",
       "2                                [TSLA]                    []   \n",
       "5                                [TSLA]                    []   \n",
       "7                                [TSLA]                    []   \n",
       "8                                [AAPL]                    []   \n",
       "9                                [TSLA]                    []   \n",
       "11                               [TSLA]                    []   \n",
       "12                               [TSLA]                    []   \n",
       "15  [MSFT, AAPL, AMZN, FB, TSLA, BRK.B]                 [HRI]   \n",
       "16                               [TSLA]                    []   \n",
       "25                               [TSLA]                    []   \n",
       "26                               [TSLA]                    []   \n",
       "27                               [TSLA]                    []   \n",
       "29                               [TSLA]                    []   \n",
       "\n",
       "    Word_count_Henry08_pos  Word_count_Henry08_neg  Word_count_LM11_pos  \\\n",
       "0                 0.003311                0.000000             0.000000   \n",
       "1                 0.000000                0.000000             0.000000   \n",
       "2                 0.000000                0.000000             0.000000   \n",
       "5                 0.000000                0.000000             0.000000   \n",
       "7                 0.000000                0.000000             0.000000   \n",
       "8                 0.008264                0.000000             0.000000   \n",
       "9                 0.000000                0.000000             0.000000   \n",
       "11                0.000000                0.010526             0.000000   \n",
       "12                0.000000                0.000000             0.000000   \n",
       "15                0.009967                0.000000             0.000000   \n",
       "16                0.000000                0.000000             0.000000   \n",
       "25                0.010638                0.000000             0.000000   \n",
       "26                0.000000                0.000000             0.010101   \n",
       "27                0.000000                0.000000             0.000000   \n",
       "29                0.000000                0.000000             0.000000   \n",
       "\n",
       "    Word_count_LM11_neg  Word_count_Hagenau13_pos  Word_count_Hagenau13_neg  \\\n",
       "0              0.000000                       0.0                  0.000000   \n",
       "1              0.007143                       0.0                  0.000000   \n",
       "2              0.000000                       0.0                  0.000000   \n",
       "5              0.000000                       0.0                  0.000000   \n",
       "7              0.013333                       0.0                  0.000000   \n",
       "8              0.016529                       0.0                  0.008264   \n",
       "9              0.000000                       0.0                  0.000000   \n",
       "11             0.000000                       0.0                  0.000000   \n",
       "12             0.000000                       0.0                  0.000000   \n",
       "15             0.003322                       0.0                  0.000000   \n",
       "16             0.000000                       0.0                  0.000000   \n",
       "25             0.000000                       0.0                  0.010638   \n",
       "26             0.000000                       0.0                  0.000000   \n",
       "27             0.000000                       0.0                  0.000000   \n",
       "29             0.000000                       0.0                  0.007143   \n",
       "\n",
       "    News_agencies_names_count  \n",
       "0                         0.0  \n",
       "1                         0.0  \n",
       "2                         0.0  \n",
       "5                         0.0  \n",
       "7                         0.0  \n",
       "8                         0.0  \n",
       "9                         0.0  \n",
       "11                        0.0  \n",
       "12                        0.0  \n",
       "15                        0.0  \n",
       "16                        0.0  \n",
       "25                        0.0  \n",
       "26                        0.0  \n",
       "27                        0.0  \n",
       "29                        0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_shorten[[\"text\", \"Mentioned_stocks_S&P500\", \"Mentioned_stocks_NYSE\", \"Word_count_Henry08_pos\",\n",
    "                  \"Word_count_Henry08_neg\", \"Word_count_LM11_pos\", \"Word_count_LM11_neg\", \"Word_count_Hagenau13_pos\", \"Word_count_Hagenau13_neg\",\"News_agencies_names_count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the new `df_tweets` onto a new csv file called `tweets_data_features_added.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for real data\n",
    "#df_tweets_shorten.to_csv(localpath + \"/data/Tweets_Preprocessed/tweets_data_features_added.csv\", index=False)\n",
    "\n",
    "# Run this during the trial run\n",
    "df_tweets_shorten.to_csv(localpath + \"/data/Tweets_Preprocessed/df_tsla_aapl_features_added.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'></a>\n",
    "### Helper Functions\n",
    "\n",
    "For brevity, we write down all the necessary but lengthy functions in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains keywords separated by space.\n",
    "                \n",
    "    Output: \n",
    "    keywords -> The set of strings, each of which is a word from the txt file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define keywords\n",
    "    keywords = []\n",
    "    \n",
    "    # This is to keep track of the word we are reading as we traverse text.\n",
    "    this_word = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \" \":    # When running into \" \", we have finished reading a word. \n",
    "            keywords.append(this_word)\n",
    "            this_word = \"\"\n",
    "        elif text[i:] == \"\\n\":   # This may occur at the end of the string.\n",
    "            break\n",
    "        else:     # With an additional letter, just add it to the current word.\n",
    "            this_word = this_word + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \" \", we will need to append the last word to keywords.\n",
    "    if this_word != \"\":\n",
    "        keywords.append(this_word)\n",
    "    \n",
    "    # Return the result in the set format.\n",
    "    return set(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keybigrams(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains key bigrams separated by \"\\n\". \n",
    "    \n",
    "    This function should work for any n-grams, given that the text file is written in the same format.\n",
    "                \n",
    "    Output: \n",
    "    keybigrams_lemm -> The set of strings, each of which is a bigram from the csv file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define keywords\n",
    "    keybigrams = []\n",
    "    \n",
    "    # This is to keep track of the word we are reading as we traverse text.\n",
    "    this_bigram = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \"\\n\":    # When running into \"\\n\", we have finished reading a bigram.\n",
    "            keybigrams.append(this_bigram)\n",
    "            this_bigram = \"\"\n",
    "        else:     # With an additional letter or space, just add it to the current bigram.\n",
    "            this_bigram = this_bigram + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \"\\n\", we will need to append the last bigram to keybigrams.\n",
    "    if this_bigram != \"\":\n",
    "        keybigrams.append(this_bigram)\n",
    "    \n",
    "    # Return the result in the set format.\n",
    "    return set(keybigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_agencies(filename: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    filename -> The file name and its directory in string, with ending included. \n",
    "                The directory must be relative to the location of this notebook.\n",
    "                This file contains names of news agencies separated by space.\n",
    "                \n",
    "    Output: \n",
    "    news_agencies -> The set of strings, each of which is a name of news agency from the input file.\n",
    "    \"\"\"\n",
    "    # Load the file into a string\n",
    "    text = open(filename, 'r').read()\n",
    "    \n",
    "    # Define news_agencies\n",
    "    news_agencies = []\n",
    "    \n",
    "    # This is to keep track of the news agency name we are reading as we traverse text.\n",
    "    this_word = \"\"\n",
    "    \n",
    "    # Traversing text\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \" \":    # When running into \" \", we have finished reading an agency's name. \n",
    "            news_agencies.append(this_word)\n",
    "            this_word = \"\"\n",
    "        elif text[i:] == \"\\n\":   # This may occur at the end of the string.\n",
    "            break\n",
    "        else:     # With an additional letter, just add it to the current agency's name.\n",
    "            this_word = this_word + text[i].lower()\n",
    "    \n",
    "    # If the string does not end in \" \", we will need to append the last agency's name to news_agencies.\n",
    "    if this_word != \"\":\n",
    "        news_agencies.append(this_word)\n",
    "    \n",
    "    # Return the result in the set format.\n",
    "    return set(news_agencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_wordcounts(tweet, keys, normalize=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    keys -> The list of sets of key words. For example, keys = [henry08_pos, henry08_neg, ..., newslib]\n",
    "            Each keyword is assumed to contain only English letter. WARNING: must remove bigrams\n",
    "    normalize -> If True, the word count for each keyword list is normalized by tweet_length.\n",
    "                 If False, the raw word count will be returned.\n",
    "    \n",
    "    Output:\n",
    "    wordcounts -> A list of length num_keys. Each element is the (normalized) word count corresponding\n",
    "                  to the number of phrases from one of the phrase lists that appear in the tweet, \n",
    "                  as reported in wordlocs.\n",
    "    \"\"\"\n",
    "    # Define a spaCy's doc object for the tweet\n",
    "    tweet_doc = nlp(tweet.lower())\n",
    "    \n",
    "    # Convert the doc object into a set of words\n",
    "    tweet_words = set([token.text for token in tweet_doc])\n",
    "    \n",
    "    # Initialize wordcounts\n",
    "    wordcounts = []\n",
    "    \n",
    "    # For each words library, we count the number of words in tweet using the more efficient \n",
    "    # intersection method. Then, if called for, we normalize the count by the length of the raw tweet.\n",
    "    for i in range(len(keys)):      # Not including the news agencies for now\n",
    "        this_wordcount = len(tweet_words.intersection(keys[i])) \n",
    "        if normalize:\n",
    "            this_wordcount_normalized = this_wordcount / len(tweet)\n",
    "            wordcounts.append(this_wordcount_normalized)\n",
    "        else:\n",
    "            wordcounts.append(this_wordcount)\n",
    "    \n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_bigramcounts(tweet, keys, normalize=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    keys -> The list of sets of key bigrams. For example, keys = [\"Hagenau13_pos\", \"Hagenau13_neg\"]\n",
    "            Each key bigram is assumed to contain only English letter and space.\n",
    "    normalize -> If True, the word count for each keyword list is normalized by tweet_length.\n",
    "                 If False, the raw word count will be returned.\n",
    "    \n",
    "    Output:\n",
    "    wordcounts -> A list of length num_keys. Each element is the (normalized) word count corresponding\n",
    "                  to the number of phrases from one of the phrase lists that appear in the tweet, \n",
    "                  as reported in wordlocs.\n",
    "    \"\"\"\n",
    "    # Define a spaCy's doc object for the tweet\n",
    "    tweet_doc = nlp(tweet.lower())\n",
    "        \n",
    "    # Convert the doc object into a list of words with stop words removed, in accordance with the bigram libraries.\n",
    "    tweet_words = [token.text for token in tweet_doc if not token.is_stop]\n",
    "    \n",
    "    # Define the set of bigrams from the tweet, consisting of pairs of neighboring words.\n",
    "    tweet_bigrams = set([tweet_words[i] + \" \" + tweet_words[i+1] for i in range(len(tweet_words) - 1)])\n",
    "    \n",
    "    # Initialize wordcounts\n",
    "    wordcounts = []\n",
    "    \n",
    "    # For each bigrams library, we count the number of bigrams in tweet_bigrams using the more efficient \n",
    "    # intersection method. Then, if called for, we normalize the count by the length of the raw tweet.\n",
    "    for i in range(len(keys)):      # Not including the news agencies for now\n",
    "        this_wordcount = len(tweet_bigrams.intersection(keys[i])) \n",
    "        if normalize:\n",
    "            this_wordcount_normalized = this_wordcount / len(tweet)\n",
    "            wordcounts.append(this_wordcount_normalized)\n",
    "        else:\n",
    "            wordcounts.append(this_wordcount)\n",
    "    \n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_list(tweet, stock_indices, company_names=[]):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    stock_indices -> The list of stock indices in string\n",
    "    company_names -> The list of company names corresponding to stock_indices\n",
    "                     If not provided, company names will not be searched for in tweet.\n",
    "    \n",
    "    Output:\n",
    "    stock_list -> A list of stock indices in stock_indices that are mentioned in tweet,\n",
    "                  either by indices or by company names.\n",
    "    \"\"\"\n",
    "    # To make this case-insensitive, make tweet all lowercase.\n",
    "    tweet_processed = tweet.lower()\n",
    "    \n",
    "    # Initialize stock_list as an empty list.\n",
    "    stock_list = []\n",
    "    \n",
    "    # For each stock index, make it lowercase then find if it appears in tweet_processed.\n",
    "    # To avoid false positives (in finding a mention), we only consider indices followed by \" \"\n",
    "    # and preceeded by \"$\" or \"#\"\n",
    "    for i in range(len(stock_indices)):\n",
    "        loc_dollar = tweet_processed.find(\"$\" + stock_indices[i].lower() + \" \")\n",
    "        loc_hashtag = tweet_processed.find(\"#\" + stock_indices[i].lower() + \" \")\n",
    "        if max(loc_dollar, loc_hashtag) >= 0:\n",
    "            stock_list.append(stock_indices[i])\n",
    "    \n",
    "    # For each company name, if available, make it lowercase then find if it appears in tweet_processed.\n",
    "    for i in range(len(company_names)):\n",
    "        loc = tweet_processed.find(\" \" + company_names[i].lower() + \" \")\n",
    "        if loc >= 0:\n",
    "            stock_list.append(stock_indices[i])\n",
    "    \n",
    "    return list(set(stock_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "The functions below are no longer used in the current version. We merely keep them in case we decide to revert the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_lemmatize(keywords):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    keywords -> The list of key words/bigrams extracted from a library file.\n",
    "    \n",
    "    Output:\n",
    "    keywords_lemm -> A list with elements from keywords, each converted into its lemma form using spaCy\n",
    "    \"\"\"\n",
    "    # Define keywords_lemm\n",
    "    keywords_lemm = []\n",
    "    \n",
    "    # Populate keywords_lemm by strings written from the lemma of the word.\n",
    "    for i in range(len(keywords)):\n",
    "        this_doc = nlp(keywords[i])\n",
    "        this_word_lemm = \"\"\n",
    "        for token in this_doc:\n",
    "            this_word_lemm = this_word_lemm + token.lemma_ + \" \"   # Add space in case there are multiple words\n",
    "        keywords_lemm.append(this_word_lemm[:-1])      # Discard the final space\n",
    "    \n",
    "    return keywords_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet_doc):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet_doc -> The raw tweet text in spaCy's doc type\n",
    "    \n",
    "    Output:\n",
    "    processed_words -> A list of words in tweet_doc with stop words (do, is, not, you, etc) removed.\n",
    "    \"\"\"\n",
    "    # Define and populate the list of all tokens in doc_raw\n",
    "    token_list = []\n",
    "    for token in tweet_doc:\n",
    "        token_list.append(token)\n",
    "    \n",
    "    # Define and write down the tweet without stop words\n",
    "    tweet_cleaned = \"\"\n",
    "    for token in token_list:\n",
    "        if not token.is_stop:\n",
    "            tweet_cleaned = tweet_cleaned + token.text + \" \"\n",
    "    \n",
    "    # Finally, convert to doc once again\n",
    "    return nlp(tweet_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_wordlocs(tweet, keys, remove_stop=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    tweet -> The raw tweet text in string\n",
    "    keys -> The list of lists of key words/bigrams. For example, keys = [henry08_pos, henry08_neg, ..., newslib]\n",
    "            Each key word/bigram is assumed to contain only English letter and space.\n",
    "    remove_stop -> If True, remove stop words (do, is, not, you, etc) from tweet as a preprocessing step.\n",
    "                   This option is recommended if the user would like to use the bigram lists by Hagenau et al (2013)\n",
    "                   because they come with stop words removed, e.g. \"able add\" assumes that \"to\" has been removed.\n",
    "    \n",
    "    Output:\n",
    "    wordlocs -> A list whose elements are of the form [keyloc, start, end], such that for the key word/bigram \n",
    "    from the i-th library, keys[i], located in tweet from tweet[j] to tweet[j+k-1] inclusive, we have \n",
    "    keyloc = i, start = j and end = j+k. The elements of wordlocs are sorted based on the value of start.\n",
    "    \"\"\"\n",
    "    # Define the case-insensitive phrase matcher.\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    \n",
    "    # Convert tweet into a spaCy's doc object with stop words removed if called for.\n",
    "    if remove_stop:\n",
    "        tweet_text = preprocess_tweet(tweet)\n",
    "    else:\n",
    "        tweet_text = nlp(tweet)\n",
    "    \n",
    "    # Tokenize the key phrases and introduce them to the phrase matcher model.\n",
    "    for i in range(len(keys)):\n",
    "        phrases = [nlp(phrase) for phrase in keys[i]]\n",
    "        matcher.add(str(i), phrases)\n",
    "    \n",
    "    # Find the matches\n",
    "    matches = matcher(tweet_text)\n",
    "    \n",
    "    # Define and populate wordlocs\n",
    "    wordlocs = []\n",
    "    for i in range(len(matches)):\n",
    "        keyloc_shifted, start, end = matches[i]\n",
    "        keyloc = int(nlp.vocab.strings[keyloc_shifted])\n",
    "        wordlocs.append([keyloc, start, end])\n",
    "    \n",
    "    return wordlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlocs_to_wordcounts(wordlocs, tweet_length, num_keys, normalize=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    wordlocs -> The list of keys and locations found in the tweet, c.f. tweet_to_wordlocs function.\n",
    "    tweet_length -> The length of tweet in words.\n",
    "    num_keys -> The number of phrase lists, i.e. len(keys) from tweet_to_wordlocs function.\n",
    "    normalize -> If True, the word count for each keyword list is normalized by tweet_length.\n",
    "                 If False, the raw word count will be returned.\n",
    "    \n",
    "    Output:\n",
    "    wordcounts -> A list of length num_keys. Each element is the (normalized) word count corresponding\n",
    "                  to the number of phrases from one of the phrase lists that appear in the tweet, \n",
    "                  as reported in wordlocs.\n",
    "    \"\"\"\n",
    "    # Define and populate wordcounts\n",
    "    wordcounts = [0 for i in range(num_keys)]\n",
    "    for j in range(len(wordlocs)):\n",
    "        wordcounts[wordlocs[j][0]] += 1\n",
    "    \n",
    "    # Perform normalization as needed\n",
    "    if normalize:\n",
    "        return [wordcounts[i]/tweet_length for i in range(num_keys)]\n",
    "    \n",
    "    # In the case where normalization is not called for\n",
    "    return wordcounts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
