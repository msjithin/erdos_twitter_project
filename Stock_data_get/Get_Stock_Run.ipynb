{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from ediblepickle import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import subprocess\n",
    "from urllib.parse import quote\n",
    "from retrying import retry\n",
    "from time import sleep\n",
    "from ediblepickle import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"MY2P7WF6CWPVBE7O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@checkpoint(key=lambda args, kwargs: quote(args[0]+'_'+args[1]) + '.pkl', work_dir=cache_dir)\n",
    "@retry\n",
    "def load_data(symbol,month):\n",
    "    CSV_URL = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol='+\\\n",
    "    symbol+'&interval=1min&slice='+month+'&apikey='+API_key\n",
    "    with requests.Session() as s:\n",
    "        download = s.get(CSV_URL)\n",
    "        decoded_content = download.content.decode('utf-8')\n",
    "        cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "        data = list(cr)\n",
    "        df = pd.DataFrame(data[1:],columns=data[0])\n",
    "        if len(data) < 10:\n",
    "            raise IOError(\"Failed attempt\")\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/josht/Documents/GitHub/erdos_twitter_project/data/Stock_indices/snp500_list.csv')\n",
    "tickers = df.Symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250    INTC\n",
       "251     ICE\n",
       "252     IBM\n",
       "253      IP\n",
       "254     IPG\n",
       "       ... \n",
       "371     PFE\n",
       "372      PM\n",
       "373     PSX\n",
       "374     PNW\n",
       "375     PXD\n",
       "Name: Symbol, Length: 126, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers[250: 376]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try aggregating the first one: `INTC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(tickers[250], \"year1month1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [1,2]:\n",
    "    for string in [f'year{j}month{k}' for k in range(3-j,13)]:\n",
    "        df2 = load_data(tickers[250], string)\n",
    "        df = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[250] + \".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try on tickers 251 - 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICE year1month4 finished\n",
      "ICE year1month8 finished\n",
      "ICE year1month12 finished\n",
      "ICE year2month4 finished\n",
      "ICE year2month8 finished\n",
      "ICE year2month12 finished\n",
      "IBM year1month4 finished\n",
      "IBM year1month8 finished\n",
      "IBM year1month12 finished\n",
      "IBM year2month4 finished\n",
      "IBM year2month8 finished\n",
      "IBM year2month12 finished\n",
      "IP year1month4 finished\n",
      "IP year1month8 finished\n",
      "IP year1month12 finished\n",
      "IP year2month4 finished\n",
      "IP year2month8 finished\n",
      "IP year2month12 finished\n",
      "IPG year1month4 finished\n",
      "IPG year1month8 finished\n",
      "IPG year1month12 finished\n",
      "IPG year2month4 finished\n",
      "IPG year2month8 finished\n",
      "IPG year2month12 finished\n",
      "IFF year1month4 finished\n",
      "IFF year1month8 finished\n",
      "IFF year1month12 finished\n",
      "IFF year2month4 finished\n",
      "IFF year2month8 finished\n",
      "IFF year2month12 finished\n",
      "INTU year1month4 finished\n",
      "INTU year1month8 finished\n",
      "INTU year1month12 finished\n",
      "INTU year2month4 finished\n",
      "INTU year2month8 finished\n",
      "INTU year2month12 finished\n",
      "ISRG year1month4 finished\n",
      "ISRG year1month8 finished\n",
      "ISRG year1month12 finished\n",
      "ISRG year2month4 finished\n",
      "ISRG year2month8 finished\n",
      "ISRG year2month12 finished\n",
      "IVZ year1month4 finished\n",
      "IVZ year1month8 finished\n",
      "IVZ year1month12 finished\n",
      "IVZ year2month4 finished\n",
      "IVZ year2month8 finished\n",
      "IVZ year2month12 finished\n",
      "IPGP year1month4 finished\n",
      "IPGP year1month8 finished\n",
      "IPGP year1month12 finished\n",
      "IPGP year2month4 finished\n",
      "IPGP year2month8 finished\n",
      "IPGP year2month12 finished\n",
      "IQV year1month4 finished\n",
      "IQV year1month8 finished\n",
      "IQV year1month12 finished\n",
      "IQV year2month4 finished\n",
      "IQV year2month8 finished\n",
      "IQV year2month12 finished\n"
     ]
    }
   ],
   "source": [
    "for i in range(251, 261):\n",
    "    df = load_data(tickers[i], \"year1month1\")\n",
    "    for j in [1,2]:\n",
    "        kk = 3 - j\n",
    "        for string in [f'year{j}month{k}' for k in range(3-j,13)]:\n",
    "            df2 = load_data(tickers[i], string)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "            if kk % 4 == 0:\n",
    "                print(tickers[i] + \" \" + string + \" finished\")\n",
    "            kk += 1\n",
    "    df.to_csv(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[i] + \".csv\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're huge. Let's turn them into parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(250, 261):\n",
    "    df = pd.read_csv(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[i] + \".csv\")\n",
    "    df.to_parquet(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[i] + \".parquet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the rest, also saving the tables as parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. PCAR finished\n",
      "2. PKG finished\n",
      "3. PH finished\n",
      "4. PAYX finished\n",
      "5. PAYC finished\n",
      "6. PYPL finished\n",
      "7. PENN finished\n",
      "8. PNR finished\n",
      "9. PBCT finished\n",
      "10. PEP finished\n",
      "11. PKI finished\n",
      "12. PFE finished\n",
      "13. PM finished\n",
      "14. PSX finished\n",
      "15. PNW finished\n"
     ]
    }
   ],
   "source": [
    "ii = 1\n",
    "for i in range(360, 375):\n",
    "    df = load_data(tickers[i], \"year1month1\")\n",
    "    for j in [1,2]:\n",
    "        for string in [f'year{j}month{k}' for k in range(3-j,13)]:\n",
    "            df2 = load_data(tickers[i], string)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "    df.to_parquet(\"/Users/josht/Documents/Stock_data_250_375/\" + tickers[i] + \".parquet\")\n",
    "    print(str(ii) + \". \" + tickers[i] + \" finished\")\n",
    "    ii += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OGN'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers[358]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OTIS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers[359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PNW'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers[374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[270, 'KSU'], [358, 'OGN'], [359, 'OTIS']]\n"
     ]
    }
   ],
   "source": [
    "Problematic_indices = [[270, \"KSU\"], [358, \"OGN\"], [359, \"OTIS\"]]\n",
    "print(Problematic_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/josht/Downloads/OTIS1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 21):\n",
    "    df2 = pd.read_csv(\"/Users/josht/Downloads/OTIS\" + str(i) + \".csv\")\n",
    "    df = pd.concat([df, df2], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-19 16:03:00</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>15603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-19 16:02:00</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>86.260000</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-19 16:00:00</td>\n",
       "      <td>86.130000</td>\n",
       "      <td>86.280000</td>\n",
       "      <td>86.120000</td>\n",
       "      <td>86.250000</td>\n",
       "      <td>85685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-19 15:59:00</td>\n",
       "      <td>86.145000</td>\n",
       "      <td>86.150000</td>\n",
       "      <td>86.110000</td>\n",
       "      <td>86.130000</td>\n",
       "      <td>21897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-19 15:58:00</td>\n",
       "      <td>86.100000</td>\n",
       "      <td>86.190000</td>\n",
       "      <td>86.075000</td>\n",
       "      <td>86.145000</td>\n",
       "      <td>22086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161178</th>\n",
       "      <td>2020-04-03 08:38:00</td>\n",
       "      <td>42.663714</td>\n",
       "      <td>42.663714</td>\n",
       "      <td>42.663714</td>\n",
       "      <td>42.663714</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161179</th>\n",
       "      <td>2020-04-03 08:29:00</td>\n",
       "      <td>42.174452</td>\n",
       "      <td>42.174452</td>\n",
       "      <td>42.174452</td>\n",
       "      <td>42.174452</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161180</th>\n",
       "      <td>2020-04-03 08:22:00</td>\n",
       "      <td>41.518840</td>\n",
       "      <td>41.518840</td>\n",
       "      <td>41.518840</td>\n",
       "      <td>41.518840</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161181</th>\n",
       "      <td>2020-04-03 08:15:00</td>\n",
       "      <td>42.017888</td>\n",
       "      <td>42.017888</td>\n",
       "      <td>42.017888</td>\n",
       "      <td>42.017888</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161182</th>\n",
       "      <td>2020-04-03 08:11:00</td>\n",
       "      <td>40.276112</td>\n",
       "      <td>40.325038</td>\n",
       "      <td>40.119646</td>\n",
       "      <td>40.325038</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161183 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time       open       high        low      close  \\\n",
       "0       2021-11-19 16:03:00  86.260000  86.260000  86.260000  86.260000   \n",
       "1       2021-11-19 16:02:00  86.260000  86.260000  86.260000  86.260000   \n",
       "2       2021-11-19 16:00:00  86.130000  86.280000  86.120000  86.250000   \n",
       "3       2021-11-19 15:59:00  86.145000  86.150000  86.110000  86.130000   \n",
       "4       2021-11-19 15:58:00  86.100000  86.190000  86.075000  86.145000   \n",
       "...                     ...        ...        ...        ...        ...   \n",
       "161178  2020-04-03 08:38:00  42.663714  42.663714  42.663714  42.663714   \n",
       "161179  2020-04-03 08:29:00  42.174452  42.174452  42.174452  42.174452   \n",
       "161180  2020-04-03 08:22:00  41.518840  41.518840  41.518840  41.518840   \n",
       "161181  2020-04-03 08:15:00  42.017888  42.017888  42.017888  42.017888   \n",
       "161182  2020-04-03 08:11:00  40.276112  40.325038  40.119646  40.325038   \n",
       "\n",
       "        volume  \n",
       "0        15603  \n",
       "1          189  \n",
       "2        85685  \n",
       "3        21897  \n",
       "4        22086  \n",
       "...        ...  \n",
       "161178     175  \n",
       "161179     389  \n",
       "161180     400  \n",
       "161181     100  \n",
       "161182    1139  \n",
       "\n",
       "[161183 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could it be that alphavantage only has the data for this index back to Sep 18, 2020? The index has existed for a longer time, but after several tries to extract it manually, noting comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"/Users/josht/Documents/Stock_data_250_375/OTIS.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"CSBIGB32V8U4NLBV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = subprocess.Popen('pwd', stdout=subprocess.PIPE)\n",
    "cmd_out, cmd_err = cmd.communicate()\n",
    "local_path = os.fsdecode(cmd_out).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@checkpoint(key=lambda args, kwargs: quote(args[0]+'_'+args[1]) + '.pkl', work_dir=cache_dir)\n",
    "@retry\n",
    "def load_data(symbol,month):\n",
    "    CSV_URL = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol='+\\\n",
    "    symbol+'&interval=1min&slice='+month+'&apikey='+API_key\n",
    "    with requests.Session() as s:\n",
    "        download = s.get(CSV_URL)\n",
    "        decoded_content = download.content.decode('utf-8')\n",
    "        cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "        data = list(cr)\n",
    "        df = pd.DataFrame(data[1:],columns=data[0])\n",
    "        if len(data) < 10:\n",
    "            raise IOError(\"Failed attempt\")\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/josht/Documents/GitHub/erdos_twitter_project/data/Stock_indices/snp500_list.csv')\n",
    "tickers = df.Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTC'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.fsencode(local_path+'/cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/cache/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(symbol):\n",
    "    df = pd.read_pickle(local_path+'/cache/'+symbol+'_year1month2.pkl')\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.startswith(f'{symbol}_') and filename.endswith('.pkl'):\n",
    "            if '_year1month2.pkl' in filename : continue  # we need to skip this file to avoid double counting\n",
    "            df = pd.concat([df,pd.read_pickle(local_path+'/cache/'+filename)])\n",
    "            #print(len(df),filename) \n",
    "            # uncomment the above line if you want some output just so that you know the code is running as expected\n",
    "            os.remove(local_path+'/cache/'+filename)\n",
    "            \n",
    "        df.to_csv(local_path+'/cache/'+f'{symbol}.csv',index=False)\n",
    "    os.remove(local_path+\"cache/\"+symbol+'_year1month2.pkl') \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/cache/INTC_year1month2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-73f9dea0dd73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/cache/INTC_year1month2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/cache/INTC_year1month2.pkl'"
     ]
    }
   ],
   "source": [
    "pd.read_pickle(local_path+'/cache/INTC_year1month2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fc3933cddcdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'year{j}month{k}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ticker in tickers[:1]:  #set the ticker range here\n",
    "    for j in [1,2]:\n",
    "        for string in [f'year{j}month{k}' for k in range(1,13)]:\n",
    "            load_data(ticker,string)\n",
    "            sleep(1)\n",
    "            \n",
    "    aggregate(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year1month1\n",
      "year1month2\n",
      "year1month3\n",
      "year1month4\n",
      "year1month5\n",
      "year1month6\n",
      "year1month7\n",
      "year1month8\n",
      "year1month9\n",
      "year1month10\n",
      "year1month11\n",
      "year1month12\n",
      "year2month1\n",
      "year2month2\n",
      "year2month3\n",
      "year2month4\n",
      "year2month5\n",
      "year2month6\n",
      "year2month7\n",
      "year2month8\n",
      "year2month9\n",
      "year2month10\n",
      "year2month11\n",
      "year2month12\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers[0:1]:  #set the ticker range here\n",
    "    for j in [1,2]:\n",
    "        for string in [f'year{j}month{k}' for k in range(1,13)]:\n",
    "            print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_data(\"TSLA\", \"year1month1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-17 20:00:00</td>\n",
       "      <td>1087.75</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>1087.6</td>\n",
       "      <td>1087.9999</td>\n",
       "      <td>2581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-17 19:59:00</td>\n",
       "      <td>1087.45</td>\n",
       "      <td>1087.96</td>\n",
       "      <td>1087.45</td>\n",
       "      <td>1087.6001</td>\n",
       "      <td>1407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-17 19:58:00</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>1087.47</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>1087.47</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-17 19:55:00</td>\n",
       "      <td>1086.79</td>\n",
       "      <td>1086.79</td>\n",
       "      <td>1086.79</td>\n",
       "      <td>1086.79</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-17 19:54:00</td>\n",
       "      <td>1087.35</td>\n",
       "      <td>1087.35</td>\n",
       "      <td>1087.35</td>\n",
       "      <td>1087.35</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17465</th>\n",
       "      <td>2021-10-19 04:05:00</td>\n",
       "      <td>875.7</td>\n",
       "      <td>876.0</td>\n",
       "      <td>875.5</td>\n",
       "      <td>875.5</td>\n",
       "      <td>2210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17466</th>\n",
       "      <td>2021-10-19 04:04:00</td>\n",
       "      <td>875.61</td>\n",
       "      <td>876.0</td>\n",
       "      <td>875.61</td>\n",
       "      <td>876.0</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17467</th>\n",
       "      <td>2021-10-19 04:03:00</td>\n",
       "      <td>875.8</td>\n",
       "      <td>875.8</td>\n",
       "      <td>875.8</td>\n",
       "      <td>875.8</td>\n",
       "      <td>1602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17468</th>\n",
       "      <td>2021-10-19 04:02:00</td>\n",
       "      <td>875.7</td>\n",
       "      <td>875.7</td>\n",
       "      <td>875.7</td>\n",
       "      <td>875.7</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17469</th>\n",
       "      <td>2021-10-19 04:01:00</td>\n",
       "      <td>874.7</td>\n",
       "      <td>874.7</td>\n",
       "      <td>874.7</td>\n",
       "      <td>874.7</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17470 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time     open     high      low      close volume\n",
       "0      2021-11-17 20:00:00  1087.75   1088.0   1087.6  1087.9999   2581\n",
       "1      2021-11-17 19:59:00  1087.45  1087.96  1087.45  1087.6001   1407\n",
       "2      2021-11-17 19:58:00   1087.0  1087.47   1087.0    1087.47   1160\n",
       "3      2021-11-17 19:55:00  1086.79  1086.79  1086.79    1086.79    762\n",
       "4      2021-11-17 19:54:00  1087.35  1087.35  1087.35    1087.35    675\n",
       "...                    ...      ...      ...      ...        ...    ...\n",
       "17465  2021-10-19 04:05:00    875.7    876.0    875.5      875.5   2210\n",
       "17466  2021-10-19 04:04:00   875.61    876.0   875.61      876.0   1764\n",
       "17467  2021-10-19 04:03:00    875.8    875.8    875.8      875.8   1602\n",
       "17468  2021-10-19 04:02:00    875.7    875.7    875.7      875.7   1110\n",
       "17469  2021-10-19 04:01:00    874.7    874.7    874.7      874.7    737\n",
       "\n",
       "[17470 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/tesla_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
