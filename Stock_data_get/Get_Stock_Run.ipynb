{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "In this notebook, we run the Python script posted in the same folder to obtain the stock price data from Alphavantage API. We aim to extract the prices from October 2019 to November 2021 of each S&P500 index. Some indices have missing entries due to the availability of data from Alphavantage. \n",
    "\n",
    "We start by importing necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from ediblepickle import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import subprocess\n",
    "from urllib.parse import quote\n",
    "from retrying import retry\n",
    "from time import sleep\n",
    "from ediblepickle import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"MY2P7WF6CWPVBE7O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@checkpoint(key=lambda args, kwargs: quote(args[0]+'_'+args[1]) + '.pkl', work_dir=cache_dir)\n",
    "@retry\n",
    "def load_data(symbol,month):\n",
    "    CSV_URL = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol='+\\\n",
    "    symbol+'&interval=1min&slice='+month+'&apikey='+API_key\n",
    "    with requests.Session() as s:\n",
    "        download = s.get(CSV_URL)\n",
    "        decoded_content = download.content.decode('utf-8')\n",
    "        cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "        data = list(cr)\n",
    "        df = pd.DataFrame(data[1:],columns=data[0])\n",
    "        if len(data) < 10:\n",
    "            raise IOError(\"Failed attempt\")\n",
    "        else:\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/josht/Documents/GitHub/erdos_twitter_project/data/Stock_indices/snp500_list.csv')\n",
    "tickers = df.Symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "\n",
    "The cells below extract the data for tickers number 250 - 374 in S&P500. Data extraction for other tickers can be done in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250    INTC\n",
       "251     ICE\n",
       "252     IBM\n",
       "253      IP\n",
       "254     IPG\n",
       "       ... \n",
       "371     PFE\n",
       "372      PM\n",
       "373     PSX\n",
       "374     PNW\n",
       "375     PXD\n",
       "Name: Symbol, Length: 126, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers[250: 376]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try aggregating the first one: `INTC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(tickers[250], \"year1month1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [1,2]:\n",
    "    for string in [f'year{j}month{k}' for k in range(3-j,13)]:\n",
    "        df2 = load_data(tickers[250], string)\n",
    "        df = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[250] + \".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try on tickers 251 - 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICE year1month4 finished\n",
      "ICE year1month8 finished\n",
      "ICE year1month12 finished\n",
      "ICE year2month4 finished\n",
      "ICE year2month8 finished\n",
      "ICE year2month12 finished\n",
      "IBM year1month4 finished\n",
      "IBM year1month8 finished\n",
      "IBM year1month12 finished\n",
      "IBM year2month4 finished\n",
      "IBM year2month8 finished\n",
      "IBM year2month12 finished\n",
      "IP year1month4 finished\n",
      "IP year1month8 finished\n",
      "IP year1month12 finished\n",
      "IP year2month4 finished\n",
      "IP year2month8 finished\n",
      "IP year2month12 finished\n",
      "IPG year1month4 finished\n",
      "IPG year1month8 finished\n",
      "IPG year1month12 finished\n",
      "IPG year2month4 finished\n",
      "IPG year2month8 finished\n",
      "IPG year2month12 finished\n",
      "IFF year1month4 finished\n",
      "IFF year1month8 finished\n",
      "IFF year1month12 finished\n",
      "IFF year2month4 finished\n",
      "IFF year2month8 finished\n",
      "IFF year2month12 finished\n",
      "INTU year1month4 finished\n",
      "INTU year1month8 finished\n",
      "INTU year1month12 finished\n",
      "INTU year2month4 finished\n",
      "INTU year2month8 finished\n",
      "INTU year2month12 finished\n",
      "ISRG year1month4 finished\n",
      "ISRG year1month8 finished\n",
      "ISRG year1month12 finished\n",
      "ISRG year2month4 finished\n",
      "ISRG year2month8 finished\n",
      "ISRG year2month12 finished\n",
      "IVZ year1month4 finished\n",
      "IVZ year1month8 finished\n",
      "IVZ year1month12 finished\n",
      "IVZ year2month4 finished\n",
      "IVZ year2month8 finished\n",
      "IVZ year2month12 finished\n",
      "IPGP year1month4 finished\n",
      "IPGP year1month8 finished\n",
      "IPGP year1month12 finished\n",
      "IPGP year2month4 finished\n",
      "IPGP year2month8 finished\n",
      "IPGP year2month12 finished\n",
      "IQV year1month4 finished\n",
      "IQV year1month8 finished\n",
      "IQV year1month12 finished\n",
      "IQV year2month4 finished\n",
      "IQV year2month8 finished\n",
      "IQV year2month12 finished\n"
     ]
    }
   ],
   "source": [
    "for i in range(251, 261):\n",
    "    df = load_data(tickers[i], \"year1month1\")\n",
    "    for j in [1,2]:\n",
    "        kk = 3 - j\n",
    "        for string in [f'year{j}month{k}' for k in range(3-j,13)]:\n",
    "            df2 = load_data(tickers[i], string)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "            if kk % 4 == 0:\n",
    "                print(tickers[i] + \" \" + string + \" finished\")\n",
    "            kk += 1\n",
    "    df.to_csv(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[i] + \".csv\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're huge. Let's turn them into parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(250, 261):\n",
    "    df = pd.read_csv(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[i] + \".csv\")\n",
    "    df.to_parquet(\"/Users/josht/Documents/GitHub/erdos_twitter_project/Stock_data_get/Stock_data_250_375/\" + tickers[i] + \".parquet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the rest, also saving the tables as parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. PCAR finished\n",
      "2. PKG finished\n",
      "3. PH finished\n",
      "4. PAYX finished\n",
      "5. PAYC finished\n",
      "6. PYPL finished\n",
      "7. PENN finished\n",
      "8. PNR finished\n",
      "9. PBCT finished\n",
      "10. PEP finished\n",
      "11. PKI finished\n",
      "12. PFE finished\n",
      "13. PM finished\n",
      "14. PSX finished\n",
      "15. PNW finished\n"
     ]
    }
   ],
   "source": [
    "ii = 1\n",
    "for i in range(360, 375):\n",
    "    df = load_data(tickers[i], \"year1month1\")\n",
    "    for j in [1,2]:\n",
    "        for string in [f'year{j}month{k}' for k in range(3-j,13)]:\n",
    "            df2 = load_data(tickers[i], string)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "    df.to_parquet(\"/Users/josht/Documents/Stock_data_250_375/\" + tickers[i] + \".parquet\")\n",
    "    print(str(ii) + \". \" + tickers[i] + \" finished\")\n",
    "    ii += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
